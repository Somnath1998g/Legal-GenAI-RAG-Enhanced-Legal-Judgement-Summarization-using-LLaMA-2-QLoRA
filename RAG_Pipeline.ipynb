{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7a775a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1389491a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dinesh/Documents/vs code/LLM_Fine-tuning/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2f06ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up directories and file paths\n",
    "preprocessed_data_dir = \"/home/dinesh/Downloads/dataset/processed-IN-Ext/\"\n",
    "train_file_A1 = os.path.join(preprocessed_data_dir, \"full_summaries_A1.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9193108d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load variables from .env into the notebook kernel\n",
    "load_dotenv()\n",
    "\n",
    "login(token=os.getenv(\"HF_TOKEN\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0ee5060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.92it/s]\n"
     ]
    }
   ],
   "source": [
    "lora_path = \"/home/dinesh/Downloads/dataset/fine_tuned_lora_model\"\n",
    "base_model = \"meta-llama/Llama-2-7b-hf\"   # <-- must be the exact base you trained on\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
    "# If your tokenizer has no pad token (common for Llama)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfcb5136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "def load_dataset_for_retrieval(jsonl_file):\n",
    "    \"\"\"\n",
    "    Load preprocessed data and extract judgments for retrieval.\n",
    "    \"\"\"\n",
    "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "\n",
    "    # Extract judgments and deduplicate\n",
    "    judgments = list(set(item[\"judgement\"].strip() for item in data))\n",
    "    return judgments\n",
    "\n",
    "# Load judgments from the file and deduplicate\n",
    "all_judgments = load_dataset_for_retrieval(train_file_A1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b79decae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert judgments to TF-IDF vectors for retrieval\n",
    "vectorizer = TfidfVectorizer()\n",
    "judgment_vectors = vectorizer.fit_transform(all_judgments).toarray().astype(np.float32)\n",
    "\n",
    "# Build FAISS index for efficient similarity search\n",
    "index = faiss.IndexFlatL2(judgment_vectors.shape[1])  # L2 distance\n",
    "index.add(judgment_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5b5b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_judgments(query, top_k=3):\n",
    "    \"\"\"\n",
    "    Retrieve top-k judgments relevant to the query using FAISS.\n",
    "    \"\"\"\n",
    "    query_vector = vectorizer.transform([query]).toarray().astype(np.float32)\n",
    "    distances, indices = index.search(query_vector, top_k)\n",
    "    return [all_judgments[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3237936",
   "metadata": {},
   "source": [
    "### Implement the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f566f18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(query, top_k=1, max_length=2048):\n",
    "    retrieved_judgments = retrieve_judgments(query, top_k=top_k)\n",
    "    summaries = []\n",
    "    input_device = next(model.parameters()).device\n",
    "\n",
    "    for judgment in retrieved_judgments:\n",
    "        input_text = (\n",
    "            \"### Instruction: Summarize the following legal text.\\n\\n\"\n",
    "            f\"### Input:\\n{judgment.strip()}\\n\\n\"\n",
    "            \"### Response:\\n\"\n",
    "        )\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=max_length,     # reduce from 4096\n",
    "            truncation=True,\n",
    "        )\n",
    "        inputs = {k: v.to(input_device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=256,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.15,\n",
    "                no_repeat_ngram_size=4,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                use_cache=True,\n",
    "            )\n",
    "\n",
    "        summary = tokenizer.decode(\n",
    "            outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "072d7e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Summaries:\n",
      "Summary 1:\n",
      "ating by way of gifts sales and exchanges a share of the properties left behind her by the husband.\n",
      "she does not acquire such powers until she has exhausted her right to remain in occupation of the properties during her lifetime.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    query = \"lakshminarayana iyer\"\n",
    "    summaries = pipeline(query, top_k=1)\n",
    "\n",
    "    print(\"Retrieved Summaries:\")\n",
    "    for i, summary in enumerate(summaries):\n",
    "        print(f\"Summary {i+1}:\\n{summary}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4cf6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

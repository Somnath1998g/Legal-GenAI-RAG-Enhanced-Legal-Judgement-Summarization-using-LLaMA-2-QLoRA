# Legal Judgment Summarization with Llama-2 (LoRA Fine-Tuning + RAG)

[![Python](https://img.shields.io/badge/Python-3.9%2B-blue.svg)](#)
[![Transformers](https://img.shields.io/badge/HuggingFace-Transformers-yellow.svg)](#)
[![PEFT](https://img.shields.io/badge/PEFT-LoRA-orange.svg)](#)
[![TRL](https://img.shields.io/badge/TRL-SFTTrainer-purple.svg)](#)

Fine-tune **Llama-2** using **LoRA (PEFT)** for **legal judgment summarization**, run inference on raw judgment text files, and optionally use a simple **RAG pipeline** (TF-IDF + FAISS) to retrieve relevant legal judgments before summarization.

---

## âœ¨ Whatâ€™s inside

- **Data preprocessing**
  - Reads raw judgments (`.txt`) and author-wise summaries
  - Produces training-ready JSONL files (e.g., `full_summaries_A1.jsonl`, `full_summaries_A2.jsonl`)
- **LoRA fine-tuning (SFT)**
  - Uses `transformers` + `trl` (`SFTTrainer`) + `peft` (LoRA)
  - Designed for low VRAM using `bitsandbytes`
- **Inference notebook**
  - Loads base Llama-2 + LoRA weights and generates summaries
- **RAG pipeline (optional)**
  - TF-IDF embeddings + FAISS similarity search
  - Retrieves top-k relevant judgments and summarizes them

---

## ğŸ“ Repository structure

```text
legal-llama-summarization/
â”œâ”€ notebooks/
â”‚  â”œâ”€ 01_Data_preprocessing.ipynb
â”‚  â”œâ”€ 02_Finetune_LLama.ipynb
â”‚  â”œâ”€ 03_Llama_inference.ipynb
â”‚  â””â”€ 04_RAG_Pipeline.ipynb
â”œâ”€ data/
â”‚  â”œâ”€ raw/IN-Ext/judgement/
â”‚  â”œâ”€ raw/IN-Ext/summary/full/
â”‚  â”œâ”€ raw/IN-Ext/summary/segment-wise/
â”‚  â””â”€ processed/processed-IN-Ext/
â”œâ”€ models/
â”‚  â”œâ”€ fine_tuned_lora_adapter/
â”‚  â””â”€ fine_tuned_lora_model/
â”œâ”€ results/runs/
â”œâ”€ requirements.txt
â”œâ”€ .env.example
â”œâ”€ .gitignore
â””â”€ README.md
```
## âœ… Requirements
- **Python 3.9+
- **NVIDIA GPU recommended for training (inference may work on CPU, but will be slow)
Main libraries:
- **transformers, datasets, trl, peft, bitsandbytes
- **For RAG: faiss-cpu, scikit-learn
## ğŸ” Hugging Face Access (Llama-2)
1. Have access approved on Hugging Face for the Llama-2 model repo
2. Use a Hugging Face token with access
Create a .env file (see .env.example) and set:
```bash
HF_TOKEN=your_huggingface_token_here
```
## ğŸš€ Setup
```bash
# 1) Clone
git clone <your-repo-url>
cd legal-llama-summarization

# 2) Create environment
python -m venv .venv
# Linux/Mac:
source .venv/bin/activate
# Windows:
# .venv\Scripts\activate

# 3) Install deps
pip install -r requirements.txt

# 4) Create env file
cp .env.example .env
# edit .env and set HF_TOKEN
```
## ğŸ§ª Notebooks workflow (recommended order)
### 1. Data preprocessing
Open:
- **notebooks/01_Data_preprocessing.ipynb
What it does:
- **Reads judgments from data/raw/IN-Ext/judgement/
- **Reads summaries from data/raw/IN-Ext/summary/full/ and segment-wise/
- **Writes JSONL to data/processed/processed-IN-Ext/
  Output example:
- **full_summaries_A1.jsonl
- **full_summaries_A2.jsonl
### 2. Fine-tune Llama-2 with LoRA (SFTTrainer)
```text
Open:
- **notebooks/02_Finetune_LLama.ipynb

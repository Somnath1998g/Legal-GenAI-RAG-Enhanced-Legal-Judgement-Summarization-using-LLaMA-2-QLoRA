# Legal Judgment Summarization with Llama-2 (LoRA Fine-Tuning + RAG)

[![Python](https://img.shields.io/badge/Python-3.9%2B-blue.svg)](#)
[![Transformers](https://img.shields.io/badge/HuggingFace-Transformers-yellow.svg)](#)
[![PEFT](https://img.shields.io/badge/PEFT-LoRA-orange.svg)](#)
[![TRL](https://img.shields.io/badge/TRL-SFTTrainer-purple.svg)](#)

Fine-tune **Llama-2** using **LoRA (PEFT)** for **legal judgment summarization**, run inference on raw judgment text files, and optionally use a simple **RAG pipeline** (TF-IDF + FAISS) to retrieve relevant legal judgments before summarization.

---

## âœ¨ Whatâ€™s inside

- **Data preprocessing**
  - Reads raw judgments (`.txt`) and author-wise summaries
  - Produces training-ready JSONL files (e.g., `full_summaries_A1.jsonl`, `full_summaries_A2.jsonl`)
- **LoRA fine-tuning (SFT)**
  - Uses `transformers` + `trl` (`SFTTrainer`) + `peft` (LoRA)
  - Designed for low VRAM using `bitsandbytes`
- **Inference notebook**
  - Loads base Llama-2 + LoRA weights and generates summaries
- **RAG pipeline (optional)**
  - TF-IDF embeddings + FAISS similarity search
  - Retrieves top-k relevant judgments and summarizes them

---

## ğŸ“ Repository structure

```text
legal-llama-summarization/
â”œâ”€ notebooks/
â”‚  â”œâ”€ 01_Data_preprocessing.ipynb
â”‚  â”œâ”€ 02_Finetune_LLama.ipynb
â”‚  â”œâ”€ 03_Llama_inference.ipynb
â”‚  â””â”€ 04_RAG_Pipeline.ipynb
â”œâ”€ data/
â”‚  â”œâ”€ raw/IN-Ext/judgement/
â”‚  â”œâ”€ raw/IN-Ext/summary/full/
â”‚  â”œâ”€ raw/IN-Ext/summary/segment-wise/
â”‚  â””â”€ processed/processed-IN-Ext/
â”œâ”€ models/
â”‚  â”œâ”€ fine_tuned_lora_adapter/
â”‚  â””â”€ fine_tuned_lora_model/
â”œâ”€ results/runs/
â”œâ”€ requirements.txt
â”œâ”€ .env.example
â”œâ”€ .gitignore
â””â”€ README.md

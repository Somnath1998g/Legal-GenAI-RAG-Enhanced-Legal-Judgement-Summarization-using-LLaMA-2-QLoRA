{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"ACCELERATE_MIXED_PRECISION\"] = \"fp16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.12/site-packages (4.4.2)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (4.57.5)\n",
      "Requirement already satisfied: trl in ./.venv/lib/python3.12/site-packages (0.26.2)\n",
      "Requirement already satisfied: peft in ./.venv/lib/python3.12/site-packages (0.18.1)\n",
      "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.12/site-packages (0.36.0)\n",
      "Requirement already satisfied: dotenv in ./.venv/lib/python3.12/site-packages (0.9.9)\n",
      "Requirement already satisfied: bitsandbytes in ./.venv/lib/python3.12/site-packages (0.49.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from datasets) (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from datasets) (2.4.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./.venv/lib/python3.12/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in ./.venv/lib/python3.12/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in ./.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.12/site-packages (from huggingface_hub) (1.2.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.12/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in ./.venv/lib/python3.12/site-packages (from trl) (1.12.0)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from peft) (7.2.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in ./.venv/lib/python3.12/site-packages (from peft) (2.9.1)\n",
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.12/site-packages (from dotenv) (1.2.1)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in ./.venv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.5.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.6.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets transformers trl peft huggingface_hub dotenv bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dinesh/Documents/vs code/LLM_Fine-tuning/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load variables from .env into the notebook kernel\n",
    "load_dotenv()\n",
    "\n",
    "login(token=os.getenv(\"HF_TOKEN\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.55s/it]\n"
     ]
    }
   ],
   "source": [
    "# Set up directories\n",
    "preprocessed_data_dir = \"/home/dinesh/Downloads/dataset/processed-IN-Ext/\"\n",
    "# Base model (gated: make sure HF login done)\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "# Where to save LoRA adapter + tokenizer\n",
    "adapter_save_dir = \"/home/dinesh/Downloads/dataset/fine_tuned_lora_adapter\"\n",
    "os.makedirs(adapter_save_dir, exist_ok=True)\n",
    " \n",
    "# (Optional) If you want a merged full model later (BIG disk usage)\n",
    "# merged_model_dir = \"../fine_tuned_lora_merged_model\"\n",
    "# Load the tokenizer and model\n",
    "# Load tokenizer + model (8-bit)\n",
    "# -------------------------\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16\n",
    ")\n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to eos_token\n",
    " \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,   # force fp16 compute\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243\n",
      "Trainable dtypes: [torch.float32]\n"
     ]
    }
   ],
   "source": [
    "# Set up LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    lora_alpha=16,          # Scaling factor for low-rank matrices\n",
    "    lora_dropout=0.1,      # Dropout rate for LoRA layers\n",
    "    r=8,                   # Rank (size of low-rank matrices)\n",
    "    bias=\"none\",           # No bias in LoRA layers\n",
    "    task_type=\"CAUSAL_LM\",  # Task type for causal language modeling\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Target modules for LoRA\n",
    ")\n",
    "# Apply LoRAg\n",
    "model = get_peft_model(model, lora_config)\n",
    "for n, p in model.named_parameters():\n",
    "    if p.requires_grad and p.dtype == torch.bfloat16:\n",
    "        p.data = p.data.to(torch.float16)\n",
    "model.print_trainable_parameters()\n",
    "# for name, p in model.named_parameters():\n",
    "#     if p.requires_grad and p.dtype == torch.bfloat16:\n",
    "#         p.data = p.data.to(torch.float16)\n",
    "print(\"Trainable dtypes:\", sorted({p.dtype for p in model.parameters() if p.requires_grad}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.float32]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dinesh/Documents/vs code/LLM_Fine-tuning/.venv/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/dinesh/Documents/vs code/LLM_Fine-tuning/.venv/lib/python3.12/site-packages/peft/mapping_func.py:78: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'meta-llama/Llama-2-7b-hf' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.\n",
      "  warnings.warn(\n",
      "/home/dinesh/Documents/vs code/LLM_Fine-tuning/.venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:285: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 3) Apply LoRA — PUT THE DTYPE WRAP HERE\n",
    "# import torch\n",
    "# from peft import get_peft_model\n",
    "\n",
    "old = torch.get_default_dtype()\n",
    "torch.set_default_dtype(torch.float16)        # force LoRA params to be created as fp16\n",
    "model = get_peft_model(model, lora_config)\n",
    "torch.set_default_dtype(old)\n",
    "\n",
    "print(sorted({p.dtype for p in model.parameters() if p.requires_grad}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Runtime guard: convert bf16 grads if they appear\n",
    "# def fp16_grad_hook(grad):\n",
    "#     if grad is not None and grad.dtype == torch.bfloat16:\n",
    "#         return grad.to(torch.float16)\n",
    "#     return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n, p in model.named_parameters():\n",
    "#     if p.requires_grad:\n",
    "#         p.register_hook(fp16_grad_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess dataset\n",
    "# -------------------------\n",
    "def load_dataset(jsonl_file):\n",
    "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    " \n",
    "    system_prompt = \"Summarize the following legal text.\"\n",
    " \n",
    "    texts = []\n",
    "    for item in data:\n",
    "        text = f\"\"\"### Instruction: {system_prompt}\n",
    " \n",
    "### Input:\n",
    "{item['judgement'].strip()[:10000]}\n",
    " \n",
    "### Response:\n",
    "{item['summary'].strip()}\n",
    "\"\"\".strip()\n",
    "        texts.append(text)\n",
    " \n",
    "    return Dataset.from_dict({\"text\": texts})\n",
    " \n",
    "train_file_A1 = os.path.join(preprocessed_data_dir, \"full_summaries_A1.jsonl\")\n",
    "train_file_A2 = os.path.join(preprocessed_data_dir, \"full_summaries_A2.jsonl\")\n",
    " \n",
    "train_dataset_A1 = load_dataset(train_file_A1)\n",
    "train_dataset_A2 = load_dataset(train_file_A2)\n",
    " \n",
    "train_data = concatenate_datasets([train_dataset_A1, train_dataset_A2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Training config (TRL 0.26.2)\n",
    "# =========================\n",
    "train_params = SFTConfig(\n",
    "    output_dir=\"/home/dinesh/Documents/vs code/results_lora\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=2e-4,      # FIX: 5e-3 is too high for LoRA on LLaMA-2\n",
    "    weight_decay=0.001,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "\n",
    "    logging_steps=50,\n",
    "    save_steps=50,\n",
    "    report_to=\"tensorboard\",\n",
    "\n",
    "    fp16=False,\n",
    "    bf16=True,              # keep bf16 off\n",
    "    max_grad_norm=0.0,       # IMPORTANT: disables clip_grad_norm_ (extra safety)\n",
    "                             # If you want clipping, set back to 0.3 AFTER this works.\n",
    "\n",
    "    group_by_length=True,\n",
    "    dataset_text_field=\"text\",\n",
    "\n",
    "         # TRL 0.26.x wants this in SFTConfig\n",
    "    packing=True,            # TRL 0.26.x wants this in SFTConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training config\n",
    "# # -------------------------\n",
    "# train_params = SFTConfig(\n",
    "#     output_dir=\"../results_lora\",\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=1,\n",
    "#     gradient_accumulation_steps=1,\n",
    "#     optim=\"paged_adamw_32bit\",\n",
    "#     save_steps=50,\n",
    "#     logging_steps=50,\n",
    "#     learning_rate=5e-3,\n",
    "#     weight_decay=0.001,\n",
    "#     fp16=True,\n",
    "#     bf16=False,\n",
    "#     bf16_full_eval=False,\n",
    "#     fp16_full_eval=False,\n",
    "#     max_grad_norm=0.3,\n",
    "#     warmup_ratio=0.03,\n",
    "#     group_by_length=True,\n",
    "#     lr_scheduler_type=\"constant\",\n",
    "#     report_to=\"tensorboard\",\n",
    "#     dataset_text_field=\"text\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding-free training is enabled, but the attention implementation is not set to a supported flash attention variant. Padding-free training flattens batches into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn2, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation` in the model configuration to one of these supported options or verify that your attention mechanism can handle flattened sequences.\n",
      "You are using packing, but the attention implementation is not set to a supported flash attention variant. Packing gathers multiple samples into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn2, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to cross-contamination between samples. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation` in the model configuration to one of these supported options.\n",
      "Adding EOS to train dataset: 100%|██████████| 100/100 [00:00<00:00, 21324.44 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 100/100 [00:00<00:00, 278.69 examples/s]\n",
      "Packing train dataset: 100%|██████████| 100/100 [00:00<00:00, 60831.09 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 03:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.638300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.543100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.348100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.274700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.967100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.923200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=1.282414639790853, metrics={'train_runtime': 193.7853, 'train_samples_per_second': 1.548, 'train_steps_per_second': 1.548, 'total_flos': 1.21941176352768e+16, 'train_loss': 1.282414639790853, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    args=train_params,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "# Start fine-tuning\n",
    "print(\"Starting fine-tuning...\")\n",
    "fine_tuning.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the fine-tuned model...\n",
      "Fine-tuned model saved at '/home/dinesh/Downloads/dataset/fine_tuned_lora_model'\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "print(\"Saving the fine-tuned model...\")\n",
    "model.save_pretrained(\"/home/dinesh/Downloads/dataset/fine_tuned_lora_model\")\n",
    "tokenizer.save_pretrained(\"/home/dinesh/Downloads/dataset/fine_tuned_lora_model\")\n",
    "print(\"Fine-tuned model saved at '/home/dinesh/Downloads/dataset/fine_tuned_lora_model'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
